# 第一节p1-p15
## p14 反向传播
![[06e919280e21a6fcc3c7e657d6d04dd.jpg]]


## p15 正则化
![[Pasted image 20230105155104.png]]
往后面添加了这项后会变得平滑，如果输入发生变化时，输出会对输入变得不敏感

## p16 分类任务
![[SmartSelect_20230125_140158_Notein  .jpg]]
![[Pasted image 20230125140028.png]]
![[Pasted image 20230125133556.png]]
![[Pasted image 20230125134434.png]]
![[Pasted image 20230125135132.png]]
通过化简最终可以得到这个式子，这就解释了class1和class2的boundary是linear
![[Pasted image 20230125135602.png]]

## p17 Logistic Regression
![[Pasted image 20230125141431.png]]
![[Pasted image 20230125141544.png]]
![[Pasted image 20230125142002.png]]
class对应y帽=1，class2对应y冒=0
![[Pasted image 20230125142027.png]]
下面这个式子就是cross entropy（cross entropy代表两个分布有多接近）
![[Pasted image 20230125142515.png]]
![[Pasted image 20230125143046.png]]
logistic regression的L(f)是likehood
![[Pasted image 20230125143324.png]]
如果用Square Error会发现距离目标远，微分就会很小，update很快可能会跑不出来
![[Pasted image 20230125143730.png]]
logistic regression的方法称为discriminative，而用高斯来描述posterior probability这件事称为generative方法，但是他们的model，function set是一样的，但是最后结果是不一样的。
![[Pasted image 20230125150644.png]]
而两个的区别就在于，generative model有做了一些假设，假设data来自某些几率模型
![[Pasted image 20230125151611.png]]

## p17 为什么是deep learning而不是fat learning?(56:26-完)
我们希望一个class的几率大于0.5另外一个小于0.5，但是现在做不到，因为logistic regression的两个class之间的boundary就是一条直线
![[Pasted image 20230125155148.png]]
![[Pasted image 20230125155314.png]]
这时可以使用Feature Transformation做转化
![[Pasted image 20230125155552.png]]
但问题是我们不知道怎么做feature transformation，我们希望机器自己产生，这时就可以把logistic regression叠起来就可以做到
![[Pasted image 20230125155748.png]]


# 第二节 p18-p30
## p18 training validation testing
因为在kaggle上的testing set分public和private可能在public的testing表现得很好但是在private表现得不好，所有可以把training set分成training set和validation set。
![[Pasted image 20230123142359.png]]

## p20 batch and momentum
主要讲为什么分batch以及batch的优缺点
所有的batch算过一边算一个epoch
![[Pasted image 20230125162829.png]]
![[Pasted image 20230125163046.png]]
但是考虑平行运算右边不一定比左边快
![[Pasted image 20230125164223.png]]
不过神奇的地方是noisy的graidient反而可以帮助training，这是optimiaztion的问题，小的batch在testing上结果更好的解释在17：20-21：30

momentum可能是可以用来解决saddle point或local minima的技术（其实就是惯性的思想）
![[Pasted image 20230125165426.png]]


## p23 看不懂..可以看林轩田的基石与技法 接p33

## p26-p27 没看

## p18 训练攻略
![[Pasted image 20230123125937.png]]
### loss在训练资料上很大
#### model bas
loss在训练资料上很大说明在训练资料上没有训练好。
一种是model bas也就是说模型太简单，在function set里面找不到那个可以让loss变小的function，这是大海捞针，但是海里没有针。可以重新设计model增加更多的feature，或者使用deep learning使model的弹性更大。
![[Pasted image 20230123131150.png]]
#### optimization
另一种是optimization没做好，这是海里有针，捞不到。比如存在最优解但是现在局部最优解中，或者鞍点，统称为critical point
如何判断是按点还是局部最优？P19 4:24-19:30
![[Pasted image 20230123151239.png]]
如何是saddle point如何解决？19：34-25：20
![[Pasted image 20230123154601.png]]
以及在低纬中可能是local minima但是其实在高维是saddle point，所以大部分都是卡在saddle point
##### momentum(p20 22:40-30:45)
momentum也是用来解决local minima或者saddle point的技术，其实就是引入了惯性的概念
![[Pasted image 20230123210959.png]]

#### 判断方法
通过比较不同的模型
![[Pasted image 20230123132459.png]]
56层的弹性肯定比2层好，但是即使在训练资料上56层的loss还是更高就说明optimization没有做好。所有可以先试试小的network看看loss，然后比较下自己的network，如果自己的network再训练资料上的loss还是更高那就是optimization的问题。

### loss在训练资料小测试资料大
#### overfitting
一种原因是flexible model也就是模型弹性很强，假设只给了三个点，模型会拟合这三个点，但是在没有训练资料作为限制的地方就会freestyle，因为他的弹性很大，所有模型会变成各种各样的function 
![[Pasted image 20230123133733.png]]
方法一是增加训练资料
![[Pasted image 20230123134027.png]]
方法二是data augmentation
方法三是限制模型，让模型的弹性低点。下面是可以限制的东西
![[Pasted image 20230123134557.png]]


#### mismatch（作业11会说到）
mismatach就是训练资料跟测试资料分布不一样，这时候增加训练资料也没有用。






## p21 Adaptive learning rate
训练卡住时，当loss不再下降的时候gradient不一定很小，大多数情况下训练还没有走到critical point训练就停止了，所以用梯度下降做optimization的时候，出现问题往往不是critical point而是其他问题
![[Pasted image 20230123215310.png]]
![[Pasted image 20230123215454.png]]
#### Root Mean Square
![[Pasted image 20230123215659.png]]
#### RMSProp
![[Pasted image 20230123220000.png]]
#### Adam:RMSPop+Momentum
#### 使用Root Mean Square
![[Pasted image 20230123220624.png]]
可以达到最优解，但是因为y轴方向会累计很多很小的阿尔法，导致最后学习率很大，会喷出去
#### 使用Learning Rate Scheduling解决RMS喷出去的问题
一个方法是learning rate decay就是随着时间减小学习率，另一个是warm up就是先变大后变小
![[Pasted image 20230123221044.png]]





# 第三节 p31-p37
## p32 为什么用了验证集还是过拟合?
使用验证集挑选模型跟训练模型类似，都是选取好的模型。所以当使用validation set来挑选模型的时候，如果模型很多那么这时就会过拟合
![[Pasted image 20230124215942.png]]
![[Pasted image 20230124220851.png]]

## p33 Spatial Transformer Layer

## p31 cnn
### 版本一
![[Pasted image 20230124134650.png]]
对于Fully Connected Network的每个neuron是要看完整的图片，就是把图片每个像素都输入给每个neuron。
但是对于图片其实只需要观察局部的特征，所以不需要每个neuron都看整张图片。

在cnn中我们会设置一个区域叫做Receptive Field，每个neuron都只关心自己的Recptive Field里面发生的事情。
![[Pasted image 20230124135552.png]]
Receptive Field可以重叠，并且同一个Recptive Field可以由不同的neuron观察（这是因为可能一个neuron没有办法观察到所有的特征所以用了多个neuron观察同个Recptive Field）
![[Pasted image 20230124140224.png]]
并且由于特征类似的特征可能出现在图片不同的位置（比如狗的脚），这时不同的Receptive Field可能会存在相同的特征，那负责检测的neuron则可以共享权值
![[Pasted image 20230124202304.png]]
共享权值的方式：同一种颜色代表两个neuron共享一样的参数，两个neuron共享一组参数，这组参数就叫做filter
![[Pasted image 20230124204122.png]]
Receptive field+Parameter sharing就是Convolutional Layer,因为Receptive field与Parameter sharing都会限制模型的弹性，所以cnn的model bias更大（model bias小，model的Flexibility就会大，就会overfitting）
![[Pasted image 20230124203217.png]]

### 版本二
通过卷积核得到堆起来的叫做Feature Map，有几个filter就会输出多数层（这时下一层的filter数量就要对应上一层输出的层数）
![[Pasted image 20230124205108.png]]
如果filter大小一直设3x3会不会让我们的network没有办法看比较大范围的特征？
不会，因为即使第二层filter大小设置3x3，但其实在原来的图像考虑的是5x5的范围，所以network越深他看的范围越大
![[Pasted image 20230124211000.png]]

### 比较
两个版本是一个意思，版本一说到的有些neuron会共用参数，这些共用的参数就是版本二的filter，而共享参数其实就是把filter扫过一张图片
![[Pasted image 20230124212052.png]]


# 第四节 p38-p47
## p38 self-attention
输入输出一样多的情况又叫做sequence labeling
经过self-attention输出的向量（有黑色框框的向量）是考虑了一整个sequence的向量。（transformer最重要的一个module就是self-attention）
![[Pasted image 20230125201324.png]]
self-attention的输入就是一串向量，
![[Pasted image 20230125202441.png]]
阿尔法代表两个向量的关联性，
![[Pasted image 20230125202708.png]]
计算阿尔法比较常见的作法是dot-product，把输入的向量分别乘以wq和wk矩阵得到qk，然后qk再做dot-product就得到了阿尔法（原理：两个向量后dot-product，结果的大小可以判断两个向量的相似度）
![[Pasted image 20230125203951.png]]
这里的soft-max换成其他的也可以
![[Pasted image 20230125205648.png]]
根据阿尔法我们已经知道了哪些向量的关联度比较高，接下来就是根据阿尔法抽取重要的信息
![[Pasted image 20230125210150.png]]

## p39 self-attention和multi-head self-attention
对运算进行简化
![[Pasted image 20230126135316.png]]
![[Pasted image 20230126140218.png]]
下面写错了a帽应该是a一撇
![[Pasted image 20230126140419.png]]
![[Pasted image 20230126140840.png]]
为什么需要multi-head self-attention?
相关这件事，在self-attention里面我们通过q和k来确定相关性，但是相关可能有很多不同的形式或定义，所以也许我们不能只有一个q而需要多个q，不同q负责不同种类的相关性
q,k,v都分别乘上两个矩阵得到qi1,qi2等，而得到两个q就代表2head
![[Pasted image 20230126142311.png]]
上面的self-attention并没有包含位置信息，如果要加入位置信息可以使用positional encoding，就是为每个位置设置一个vector就是e，至于e如何产生取决于你自己
![[Pasted image 20230126142804.png]]
self-attention和cnn的比较
![[Pasted image 20230126145446.png]]
![[Pasted image 20230126145514.png]]

## p40 Recurrent Neural Network
recurrent neural networ的角色很大一部分都可以用self-attention来代替
![[Pasted image 20230126150409.png]]

## p44 Unsupervised Learning:Word Embedding
Word Embedding就是把每一个word都project到一个high dimensional的space上面
![[Pasted image 20230126163749.png]]
![[Pasted image 20230126171042.png]]
![[Pasted image 20230126171813.png]]

## p42p43 GNN（不是李讲）
self-attention用在graph上是gnn的一个类型
![[Pasted image 20230126151048.png]]
![[Pasted image 20230126154653.png]]
如何在graph这样的data上做convolution:1.Spatial-based 2.Spectral-based
![[Pasted image 20230126154855.png]]

### Spatial-based GNN
![[Pasted image 20230126160143.png]]

### Spectral-based GNN